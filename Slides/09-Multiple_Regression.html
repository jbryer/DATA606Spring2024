<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Multiple Linear Regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jason Bryer, Ph.D.Â and Angela Lui, Ph.D." />
    <meta name="date" content="2024-04-17" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
    <link rel="stylesheet" href="assets/mtheme_max.css" type="text/css" />
    <link rel="stylesheet" href="assets/fonts_mtheme_max.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: center, middle, inverse, title-slide

# Multiple Linear Regression
## DATA 606 - Statistics &amp; Probability for Data Analytics
### Jason Bryer, Ph.D. and Angela Lui, Ph.D.
### April 17, 2024

---

# One Minute Paper Results



.pull-left[
**What was the most important thing you learned during this class?**
&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[
**What important question remains unanswered for you?**
&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;
]



---
# Weight of Books


```r
allbacks &lt;- read.csv('../course_data/allbacks.csv')
head(allbacks)
```

```
##   X volume area weight cover
## 1 1    885  382    800    hb
## 2 2   1016  468    950    hb
## 3 3   1125  387   1050    hb
## 4 4    239  371    350    hb
## 5 5    701  371    750    hb
## 6 6    641  367    600    hb
```

From: Maindonald, J.H. &amp; Braun, W.J. (2007). *Data Analysis and Graphics Using R, 2nd ed.*

---
# Weights of Books (cont) 


```r
lm.out &lt;- lm(weight ~ volume, data=allbacks)
```

$$ \hat{weight} = 108 + 0.71 volume $$
$$ R^2 = 80\% $$

&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

---
# Modeling weights of books using volume

.code70[

```r
summary(lm.out)
```

```
## 
## Call:
## lm(formula = weight ~ volume, data = allbacks)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -189.97 -109.86   38.08  109.73  145.57 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 107.67931   88.37758   1.218    0.245    
## volume        0.70864    0.09746   7.271 6.26e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 123.9 on 13 degrees of freedom
## Multiple R-squared:  0.8026,	Adjusted R-squared:  0.7875 
## F-statistic: 52.87 on 1 and 13 DF,  p-value: 6.262e-06
```
]

---
# Weights of hardcover and paperback books 

- Can you identify a trend in the relationship between volume and weight of hardcover and paperback books?

&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

--

- Paperbacks generally weigh less than hardcover books after controlling for book's volume.

---
# Modeling using volume and cover type

.code70[

```r
lm.out2 &lt;- lm(weight ~ volume + cover, data=allbacks)
summary(lm.out2)
```

```
## 
## Call:
## lm(formula = weight ~ volume + cover, data = allbacks)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -110.10  -32.32  -16.10   28.93  210.95 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  197.96284   59.19274   3.344 0.005841 ** 
## volume         0.71795    0.06153  11.669  6.6e-08 ***
## coverpb     -184.04727   40.49420  -4.545 0.000672 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 78.2 on 12 degrees of freedom
## Multiple R-squared:  0.9275,	Adjusted R-squared:  0.9154 
## F-statistic: 76.73 on 2 and 12 DF,  p-value: 1.455e-07
```
]

---
# Linear Model

$$ \hat{weight} = 198 + 0.72 volume - 184 coverpb $$

1. For **hardcover** books: plug in *0* for cover.  

`$$\hat{weight} = 197.96 + 0.72 volume - 184.05 \times 0 = 197.96 + 0.72 volume$$`

2. For **paperback** books: put in 1 for cover.
`$$\hat{weight} = 197.96 + 0.72 volume - 184.05 \times 1$$`

---
# Visualizing the linear model 

&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;


---
# Interpretation of the regression coefficients

&lt;center&gt;
&lt;!-- html table generated in R 4.3.3 by xtable 1.8-4 package --&gt;
&lt;!-- Wed Apr 17 18:27:31 2024 --&gt;
&lt;table border=1&gt;
&lt;tr&gt; &lt;th&gt;  &lt;/th&gt; &lt;th&gt; Estimate &lt;/th&gt; &lt;th&gt; Std. Error &lt;/th&gt; &lt;th&gt; t value &lt;/th&gt; &lt;th&gt; Pr(&amp;gt;|t|) &lt;/th&gt;  &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; (Intercept) &lt;/td&gt; &lt;td align="right"&gt; 197.9628 &lt;/td&gt; &lt;td align="right"&gt; 59.1927 &lt;/td&gt; &lt;td align="right"&gt; 3.34 &lt;/td&gt; &lt;td align="right"&gt; 0.0058 &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; volume &lt;/td&gt; &lt;td align="right"&gt; 0.7180 &lt;/td&gt; &lt;td align="right"&gt; 0.0615 &lt;/td&gt; &lt;td align="right"&gt; 11.67 &lt;/td&gt; &lt;td align="right"&gt; 0.0000 &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; coverpb &lt;/td&gt; &lt;td align="right"&gt; -184.0473 &lt;/td&gt; &lt;td align="right"&gt; 40.4942 &lt;/td&gt; &lt;td align="right"&gt; -4.55 &lt;/td&gt; &lt;td align="right"&gt; 0.0007 &lt;/td&gt; &lt;/tr&gt;
   &lt;/table&gt;
&lt;/center&gt;

* **Slope of volume**: All else held constant, books that are 1 more cubic centimeter in volume tend to weigh about 0.72 grams more.
* **Slope of cover**: All else held constant, the model predicts that paperback books weigh 184 grams lower than hardcover books.
* **Intercept**: Hardcover books with no volume are expected on average to weigh 198 grams.
	* Obviously, the intercept does not make sense in context. It only serves to adjust the height of the line.

---
# Modeling Poverty


```r
poverty &lt;- read.table("../course_data/poverty.txt", h = T, sep = "\t")
names(poverty) &lt;- c("state", "metro_res", "white", "hs_grad", "poverty", "female_house")
poverty &lt;- poverty[,c(1,5,2,3,4,6)]
head(poverty)
```

```
##        state poverty metro_res white hs_grad female_house
## 1    Alabama    14.6      55.4  71.3    79.9         14.2
## 2     Alaska     8.3      65.6  70.8    90.6         10.8
## 3    Arizona    13.3      88.2  87.7    83.8         11.1
## 4   Arkansas    18.0      52.5  81.0    80.9         12.1
## 5 California    12.8      94.4  77.5    81.1         12.6
## 6   Colorado     9.4      84.5  90.2    88.7          9.6
```

From: Gelman, H. (2007). *Data Analysis using Regression and Multilevel/Hierarchial Models.* Cambridge University Press.

---
# Modeling Poverty 



&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;

---
# Predicting Poverty using Percent Female Householder

.code70[

```r
lm.poverty &lt;- lm(poverty ~ female_house, data=poverty)
summary(lm.poverty)
```

```
## 
## Call:
## lm(formula = poverty ~ female_house, data = poverty)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.7537 -1.8252 -0.0375  1.5565  6.3285 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    3.3094     1.8970   1.745   0.0873 .  
## female_house   0.6911     0.1599   4.322 7.53e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.664 on 49 degrees of freedom
## Multiple R-squared:  0.276,	Adjusted R-squared:  0.2613 
## F-statistic: 18.68 on 1 and 49 DF,  p-value: 7.534e-05
```
]

---
# % Poverty by % Female Household

&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;


---
# Another look at `\(R^2\)`

`\(R^2\)` can be calculated in three ways:

1. square the correlation coefficient of x and y (how we have been
calculating it)
2. square the correlation coefficient of y and `\(\hat{y}\)` 
3. based on definition:  
$$ R^2 = \frac{explained \quad variability \quad in \quad y}{total \quad variability \quad in \quad y} $$

Using ANOVA we can calculate the explained variability and total variability in y.

---
# Sum of Squares


```r
anova.poverty &lt;- anova(lm.poverty)
print(xtable::xtable(anova.poverty, digits = 2), type='html')
```

&lt;!-- html table generated in R 4.3.3 by xtable 1.8-4 package --&gt;
&lt;!-- Wed Apr 17 18:27:32 2024 --&gt;
&lt;table border=1&gt;
&lt;tr&gt; &lt;th&gt;  &lt;/th&gt; &lt;th&gt; Df &lt;/th&gt; &lt;th&gt; Sum Sq &lt;/th&gt; &lt;th&gt; Mean Sq &lt;/th&gt; &lt;th&gt; F value &lt;/th&gt; &lt;th&gt; Pr(&amp;gt;F) &lt;/th&gt;  &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; female_house &lt;/td&gt; &lt;td align="right"&gt; 1.00 &lt;/td&gt; &lt;td align="right"&gt; 132.57 &lt;/td&gt; &lt;td align="right"&gt; 132.57 &lt;/td&gt; &lt;td align="right"&gt; 18.68 &lt;/td&gt; &lt;td align="right"&gt; 0.00 &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Residuals &lt;/td&gt; &lt;td align="right"&gt; 49.00 &lt;/td&gt; &lt;td align="right"&gt; 347.68 &lt;/td&gt; &lt;td align="right"&gt; 7.10 &lt;/td&gt; &lt;td align="right"&gt;  &lt;/td&gt; &lt;td align="right"&gt;  &lt;/td&gt; &lt;/tr&gt;
   &lt;/table&gt;

--

Sum of squares of *y*: `\({ SS }_{ Total }=\sum { { \left( y-\bar { y }  \right)  }^{ 2 } } =480.25\)` &amp;rarr; **total variability**

Sum of squares of residuals: `\({ SS }_{ Error }=\sum { { e }_{ i }^{ 2 } } =347.68\)` &amp;rarr; **unexplained variability**

Sum of squares of *x*: `\({ SS }_{ Model }={ SS }_{ Total }-{ SS }_{ Error } = 132.57\)` &amp;rarr; **explained variability**  

$$ R^2 = \frac{explained \quad variability \quad in \quad y}{total \quad variability \quad in \quad y} = \frac{132.57}{480.25} = 0.28 $$

---
# Why bother?

* For single-predictor linear regression, having three ways to calculate the same value may seem like overkill.

* However, in multiple linear regression, we can't calculate `\(R^2\)` as the square of the correlation between *x* and *y* because we have multiple *x*s.

* And next we'll learn another measure of explained variability, *adjusted `\(R^2\)`*, that requires the use of the third approach, ratio of explained and unexplained variability.

---
# Predicting poverty using % female household &amp; % white

.pull-left[.code70[

```r
lm.poverty2 &lt;- lm(poverty ~ female_house + white, data=poverty)
print(xtable::xtable(lm.poverty2), type='html')
```

&lt;!-- html table generated in R 4.3.3 by xtable 1.8-4 package --&gt;
&lt;!-- Wed Apr 17 18:27:32 2024 --&gt;
&lt;table border=1&gt;
&lt;tr&gt; &lt;th&gt;  &lt;/th&gt; &lt;th&gt; Estimate &lt;/th&gt; &lt;th&gt; Std. Error &lt;/th&gt; &lt;th&gt; t value &lt;/th&gt; &lt;th&gt; Pr(&amp;gt;|t|) &lt;/th&gt;  &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; (Intercept) &lt;/td&gt; &lt;td align="right"&gt; -2.5789 &lt;/td&gt; &lt;td align="right"&gt; 5.7849 &lt;/td&gt; &lt;td align="right"&gt; -0.45 &lt;/td&gt; &lt;td align="right"&gt; 0.6577 &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; female_house &lt;/td&gt; &lt;td align="right"&gt; 0.8869 &lt;/td&gt; &lt;td align="right"&gt; 0.2419 &lt;/td&gt; &lt;td align="right"&gt; 3.67 &lt;/td&gt; &lt;td align="right"&gt; 0.0006 &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; white &lt;/td&gt; &lt;td align="right"&gt; 0.0442 &lt;/td&gt; &lt;td align="right"&gt; 0.0410 &lt;/td&gt; &lt;td align="right"&gt; 1.08 &lt;/td&gt; &lt;td align="right"&gt; 0.2868 &lt;/td&gt; &lt;/tr&gt;
   &lt;/table&gt;
] ]
.pull-right[.code70[

```r
anova.poverty2 &lt;- anova(lm.poverty2)
print(xtable::xtable(anova.poverty2, digits = 3), type='html')
```

&lt;!-- html table generated in R 4.3.3 by xtable 1.8-4 package --&gt;
&lt;!-- Wed Apr 17 18:27:32 2024 --&gt;
&lt;table border=1&gt;
&lt;tr&gt; &lt;th&gt;  &lt;/th&gt; &lt;th&gt; Df &lt;/th&gt; &lt;th&gt; Sum Sq &lt;/th&gt; &lt;th&gt; Mean Sq &lt;/th&gt; &lt;th&gt; F value &lt;/th&gt; &lt;th&gt; Pr(&amp;gt;F) &lt;/th&gt;  &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; female_house &lt;/td&gt; &lt;td align="right"&gt; 1.000 &lt;/td&gt; &lt;td align="right"&gt; 132.568 &lt;/td&gt; &lt;td align="right"&gt; 132.568 &lt;/td&gt; &lt;td align="right"&gt; 18.745 &lt;/td&gt; &lt;td align="right"&gt; 0.000 &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; white &lt;/td&gt; &lt;td align="right"&gt; 1.000 &lt;/td&gt; &lt;td align="right"&gt; 8.207 &lt;/td&gt; &lt;td align="right"&gt; 8.207 &lt;/td&gt; &lt;td align="right"&gt; 1.160 &lt;/td&gt; &lt;td align="right"&gt; 0.287 &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td&gt; Residuals &lt;/td&gt; &lt;td align="right"&gt; 48.000 &lt;/td&gt; &lt;td align="right"&gt; 339.472 &lt;/td&gt; &lt;td align="right"&gt; 7.072 &lt;/td&gt; &lt;td align="right"&gt;  &lt;/td&gt; &lt;td align="right"&gt;  &lt;/td&gt; &lt;/tr&gt;
   &lt;/table&gt;
] ]

&lt;br/&gt;

$$ R^2 = \frac{explained \quad variability \quad in \quad y}{total \quad variability \quad in \quad y} = \frac{132.57 + 8.21}{480.25} = 0.29 $$

---
# Unique information

.left-column[Does adding the variable `white` to the model add valuable information that wasn't provided by `female_house`?]

&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;

---
# Collinearity between explanatory variables

poverty vs % female head of household

&lt;!-- html table generated in R 4.3.3 by xtable 1.8-4 package --&gt;
&lt;!-- Wed Apr 17 18:27:34 2024 --&gt;
&lt;table border=1&gt;
&lt;tr&gt; &lt;th&gt;  &lt;/th&gt; &lt;th&gt; Estimate &lt;/th&gt; &lt;th&gt; Std. Error &lt;/th&gt; &lt;th&gt; t value &lt;/th&gt; &lt;th&gt; Pr(&amp;gt;|t|) &lt;/th&gt;  &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; (Intercept) &lt;/td&gt; &lt;td align="right"&gt; 3.3094 &lt;/td&gt; &lt;td align="right"&gt; 1.8970 &lt;/td&gt; &lt;td align="right"&gt; 1.74 &lt;/td&gt; &lt;td align="right"&gt; 0.0873 &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; female_house &lt;/td&gt; &lt;td align="right"&gt; 0.6911 &lt;/td&gt; &lt;td align="right"&gt; 0.1599 &lt;/td&gt; &lt;td align="right"&gt; 4.32 &lt;/td&gt; &lt;td align="right"&gt; 0.0001 &lt;/td&gt; &lt;/tr&gt;
   &lt;/table&gt;

poverty vs % female head of household and % female household

&lt;!-- html table generated in R 4.3.3 by xtable 1.8-4 package --&gt;
&lt;!-- Wed Apr 17 18:27:34 2024 --&gt;
&lt;table border=1&gt;
&lt;tr&gt; &lt;th&gt;  &lt;/th&gt; &lt;th&gt; Estimate &lt;/th&gt; &lt;th&gt; Std. Error &lt;/th&gt; &lt;th&gt; t value &lt;/th&gt; &lt;th&gt; Pr(&amp;gt;|t|) &lt;/th&gt;  &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; (Intercept) &lt;/td&gt; &lt;td align="right"&gt; -2.5789 &lt;/td&gt; &lt;td align="right"&gt; 5.7849 &lt;/td&gt; &lt;td align="right"&gt; -0.45 &lt;/td&gt; &lt;td align="right"&gt; 0.6577 &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; female_house &lt;/td&gt; &lt;td align="right"&gt; 0.8869 &lt;/td&gt; &lt;td align="right"&gt; 0.2419 &lt;/td&gt; &lt;td align="right"&gt; 3.67 &lt;/td&gt; &lt;td align="right"&gt; 0.0006 &lt;/td&gt; &lt;/tr&gt;
  &lt;tr&gt; &lt;td align="right"&gt; white &lt;/td&gt; &lt;td align="right"&gt; 0.0442 &lt;/td&gt; &lt;td align="right"&gt; 0.0410 &lt;/td&gt; &lt;td align="right"&gt; 1.08 &lt;/td&gt; &lt;td align="right"&gt; 0.2868 &lt;/td&gt; &lt;/tr&gt;
   &lt;/table&gt;

Note the difference in the estimate for `female_house`.

---
# Collinearity between explanatory variables

* Two predictor variables are said to be collinear when they are correlated, and this collinearity complicates model estimation.  
Remember: Predictors are also called explanatory or independent variables. Ideally, they would be independent of each other.

* We don't like adding predictors that are associated with each other to the model, because often times the addition of such variable brings nothing to the table. Instead, we prefer the simplest best model, i.e. *parsimonious* model.

* While it's impossible to avoid collinearity from arising in observational data, experiments are usually designed to prevent correlation among predictors

---
# `\(R^2\)` vs. adjusted `\(R^2\)`

Model                      | `\(R^2\)` | Adjusted `\(R^2\)`
---------------------------|-------|----------------
Model 1 (Single-predictor) | 0.28  | 0.26
Model 2 (Multiple)         | 0.29  | 0.26

* When any variable is added to the model `\(R^2\)` increases.
* But if the added variable doesn't really provide any new information, or is completely unrelated, adjusted `\(R^2\)` does not increase.

---
# Adjusted `\(R^2\)`

`$${ R }_{ adj }^{ 2 }={ 1-\left( \frac { { SS }_{ error } }{ { SS }_{ total } } \times \frac { n-1 }{ n-p-1 }  \right)  }$$`

where *n* is the number of cases and *p* is the number of predictors (explanatory variables) in the model.

* Because *p* is never negative, `\({ R }_{ adj }^{ 2 }\)` will always be smaller than `\(R^2\)`.
* `\({ R }_{ adj }^{ 2 }\)` applies a penalty for the number of predictors included in the model.
* Therefore, we choose models with higher `\({ R }_{ adj }^{ 2 }\)` over others.

---
class: inverse, middle, center
# Predictive Modeling

---
# Example: Hours Studying Predicting Passing


```r
study &lt;- data.frame(
	Hours=c(0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,
			3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50),
	Pass=c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1)
)
study[sample(nrow(study), 5),]
```

```
##    Hours Pass
## 19  5.00    1
## 9   2.25    1
## 15  4.00    1
## 16  4.25    1
## 3   1.00    0
```

```r
tab &lt;- describeBy(study$Hours, group = study$Pass, mat = TRUE, skew = FALSE)
tab$group1 &lt;- as.integer(as.character(tab$group1))
```


---
# Prediction

Odds (or probability) of passing if studied **zero** hours?

`$$log(\frac{p}{1-p}) = -4.078 + 1.505 \times 0$$`
`$$\frac{p}{1-p} = exp(-4.078) = 0.0169$$`
`$$p = \frac{0.0169}{1.169} = .016$$`

--

Odds (or probability) of passing if studied **4** hours?

`$$log(\frac{p}{1-p}) = -4.078 + 1.505 \times 4$$`
`$$\frac{p}{1-p} = exp(1.942) = 6.97$$`
`$$p = \frac{6.97}{7.97} = 0.875$$`

---
# Fitted Values


```r
study[1,]
```

```
##   Hours Pass
## 1   0.5    0
```

```r
logistic &lt;- function(x, b0, b1) {
	return(1 / (1 + exp(-1 * (b0 + b1 * x)) ))
}
logistic(.5, b0=-4.078, b1=1.505)
```

```
## [1] 0.03470667
```

---
# Model Performance

The use of statistical models to predict outcomes, typically on new data, is called predictive modeling. Logistic regression is a common statistical procedure used for prediction. We will utilize a **confusion matrix** to evaluate accuracy of the predictions.

&lt;img src="images/Confusion_Matrix.png" width="1100" style="display: block; margin: auto;" /&gt;

---
class: font80
# Predicting Heart Attacks

Source: https://www.kaggle.com/datasets/imnikhilanand/heart-attack-prediction?select=data.csv


```r
heart &lt;- read.csv('../course_data/heart_attack_predictions.csv')
heart &lt;- heart |&gt;
	mutate_if(is.character, as.numeric) |&gt;
	select(!c(slope, ca, thal))
str(heart)
```

```
## 'data.frame':	294 obs. of  11 variables:
##  $ age     : int  28 29 29 30 31 32 32 32 33 34 ...
##  $ sex     : int  1 1 1 0 0 0 1 1 1 0 ...
##  $ cp      : int  2 2 2 1 2 2 2 2 3 2 ...
##  $ trestbps: num  130 120 140 170 100 105 110 125 120 130 ...
##  $ chol    : num  132 243 NA 237 219 198 225 254 298 161 ...
##  $ fbs     : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ restecg : num  2 0 0 1 1 0 0 0 0 0 ...
##  $ thalach : num  185 160 170 170 150 165 184 155 185 190 ...
##  $ exang   : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ oldpeak : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ num     : int  0 0 0 0 0 0 0 0 0 0 ...
```

Note: `num` is the diagnosis of heart disease (angiographic disease status) (i.e. Value 0: &lt; 50% diameter narrowing -- Value 1: &gt; 50% diameter narrowing)


---
# Missing Data

We will save this for another day...


```r
complete.cases(heart) |&gt; table()
```

```
## 
## FALSE  TRUE 
##    33   261
```

```r
mice_out &lt;- mice::mice(heart, m = 1)
```

```
## 
##  iter imp variable
##   1   1  trestbps  chol  fbs  restecg  thalach  exang
##   2   1  trestbps  chol  fbs  restecg  thalach  exang
##   3   1  trestbps  chol  fbs  restecg  thalach  exang
##   4   1  trestbps  chol  fbs  restecg  thalach  exang
##   5   1  trestbps  chol  fbs  restecg  thalach  exang
```

```r
heart &lt;- mice::complete(mice_out)
```


---
# Data Setup

We will split the data into a training set (70% of observations) and validation set (30%).


```r
train.rows &lt;- sample(nrow(heart), nrow(heart) * .7)
heart_train &lt;- heart[train.rows,]
heart_test &lt;- heart[-train.rows,]
```

This is the proportions of survivors and defines what our "guessing" rate is. That is, if we guessed no one had a heart attack, we would be correct 62% of the time.


```r
(heart_attack &lt;- table(heart_train$num) %&gt;% prop.table)
```

```
## 
##         0         1 
## 0.6243902 0.3756098
```

---
class: font80
# Model Training


```r
lr.out &lt;- glm(num ~ ., data=heart_train, family=binomial(link = 'logit'))
summary(lr.out)
```

```
## 
## Call:
## glm(formula = num ~ ., family = binomial(link = "logit"), data = heart_train)
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -4.177722   3.463954  -1.206 0.227796    
## age         -0.033228   0.033254  -0.999 0.317689    
## sex          1.314556   0.563216   2.334 0.019595 *  
## cp           0.934521   0.266109   3.512 0.000445 ***
## trestbps     0.008442   0.014618   0.578 0.563591    
## chol         0.003300   0.003131   1.054 0.291781    
## fbs          2.016907   0.789637   2.554 0.010643 *  
## restecg     -0.167309   0.524195  -0.319 0.749594    
## thalach     -0.013756   0.011142  -1.235 0.216970    
## exang        1.065755   0.508887   2.094 0.036234 *  
## oldpeak      1.027296   0.273848   3.751 0.000176 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 271.37  on 204  degrees of freedom
## Residual deviance: 147.41  on 194  degrees of freedom
## AIC: 169.41
## 
## Number of Fisher Scoring iterations: 5
```

---
# Predicted Values


```r
heart_train$prediction &lt;- predict(lr.out, type = 'response', newdata = heart_train)
ggplot(heart_train, aes(x = prediction, color = num == 1)) + geom_density()
```

&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-31-1.png" style="display: block; margin: auto;" /&gt;

---
# Results


```r
heart_train$prediction_class &lt;- heart_train$prediction &gt; 0.5
tab &lt;- table(heart_train$prediction_class, 
			 heart_train$num) %&gt;% prop.table() %&gt;% print()
```

```
##        
##                  0          1
##   FALSE 0.56585366 0.09268293
##   TRUE  0.05853659 0.28292683
```

For the training set, the overall accuracy is 84.88%. Recall that 62.44% people did not have a heart attach. Therefore, the simplest model would be to predict that no one had a heart attack, which would mean we would be correct 62.44% of the time. Therefore, our prediction model is 22.44% better than guessing.

---
# Checking with the validation dataset


```r
(survived_test &lt;- table(heart_test$num) %&gt;% prop.table())
```

```
## 
##         0         1 
## 0.6741573 0.3258427
```

```r
heart_test$prediction &lt;- predict(lr.out, newdata = heart_test, type = 'response')
heart_test$prediciton_class &lt;- heart_test$prediction &gt; 0.5
tab_test &lt;- table(heart_test$prediciton_class, heart_test$num) %&gt;%
	prop.table() %&gt;% print()
```

```
##        
##                  0          1
##   FALSE 0.62921348 0.11235955
##   TRUE  0.04494382 0.21348315
```

The overall accuracy is 84.27%, or 16.9% better than guessing.

---
# Receiver Operating Characteristic (ROC) Curve

The ROC curve is created by plotting the true positive rate (TPR; AKA sensitivity) against the false positive rate (FPR; AKA probability of false alarm) at various threshold settings.


```r
roc &lt;- calculate_roc(heart_train$prediction, heart_train$num == 1)
summary(roc)
```

```
## AUC = 0.909
## Cost of false-positive = 1
## Cost of false-negative = 1
## Threshold with minimum cost = 0.646
```

---
# ROC Curve


```r
plot(roc)
```

&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-35-1.png" style="display: block; margin: auto;" /&gt;

---
# ROC Curve


```r
plot(roc, curve = 'accuracy')
```

&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-36-1.png" style="display: block; margin: auto;" /&gt;

---
class: font90
# Caution on Interpreting Accuracy

- [Loh, Sooo, and Zing](http://cs229.stanford.edu/proj2016/report/LohSooXing-PredictingSexualOrientationBasedOnFacebookStatusUpdates-report.pdf) (2016) predicted sexual orientation based on Facebook Status.

- They reported model accuracies of approximately 90% using SVM, logistic regression and/or random forest methods.

--

- [Gallup](https://news.gallup.com/poll/234863/estimate-lgbt-population-rises.aspx) (2018) poll estimates that 4.5% of the Americal population identifies as LGBT.

--

- *My proposed model:* I predict all Americans are heterosexual.

- The accuracy of my model is 95.5%, or *5.5% better than Facebook's model!*

- Predicting "rare" events (i.e. when the proportion of one of the two outcomes large) is difficult and requires independent (predictor) variables that strongly associated with the dependent (outcome) variable.

---
# Fitted Values Revisited 

What happens when the ratio of true-to-false increases (i.e. want to predict "rare" events)?

Let's simulate a dataset where the ratio of true-to-false is 10-to-1. We can also define the distribution of the dependent variable. Here, there is moderate separation in the distributions.




```r
test.df2 &lt;- getSimulatedData(
	treat.mean=.6, control.mean=.4)
```

The `multilevelPSA::psrange` function will sample with varying ratios from 1:10 to 1:1. It takes multiple samples and averages the ranges and distributions of the fitted values from logistic regression.


```r
psranges2 &lt;- psrange(test.df2, test.df2$treat, treat ~ .,
					 samples=seq(100,1000,by=100), nboot=20)
```

---
# Fitted Values Revisited (cont.)


```r
plot(psranges2)
```

&lt;img src="09-Multiple_Regression_files/figure-html/unnamed-chunk-40-1.png" style="display: block; margin: auto;" /&gt;

---
# Additional Resources


* [The Path to Log Likelihood](https://jbryer.github.io/VisualStats/articles/log_likelihood.html)

* [Visual Introduction to Maximum Likelihood Estimation](https://jbryer.github.io/VisualStats/articles/mle.html)

* [VisualStats R Package](https://jbryer.github.io/VisualStats/index.html)

* [Logistic Regression Details Pt 2: Maximum Likelihood](https://www.youtube.com/watch?v=BfKanl1aSG0)

* [StatQuest: Maximum Likelihood, clearly explained](https://www.youtube.com/watch?v=XepXtl9YKwc)

* [Probability concepts explained: Maximum likelihood estimation](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1)

---
class: left, font140
# One Minute Paper

Complete the one minute paper: 
https://forms.gle/Jcw55CYvc6Ym8A5F7

1. What was the most important thing you learned during this class?
2. What important question remains unanswered for you?

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "solarized-light",
"highlightLanguage": "R",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<!-- Source: https://www.garrickadenbuie.com/blog/xaringan-tip-logo-all-slides/ -->
<style>
.logo {
  background-image: url(images/hex/DATA606.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  bottom: 2em;
  right: 0.5em;
  width: 55px;
  height: 64px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
